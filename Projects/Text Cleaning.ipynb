{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d445121-d5d8-4f6b-85fb-876261285264",
   "metadata": {},
   "source": [
    "## Working with text data - Text Preprocessing\n",
    "\n",
    "### **Text preprocessing steps:**\n",
    "\n",
    "Text preprocessing steps include some essential tasks to clean and remove the noise from the available data.\n",
    "\n",
    "1. **Removing special characters and punctuations.**\n",
    "2. **Converting to lowercase**\n",
    "3. **Tokenization(Sentence tokenization and word tokenization)**\n",
    "4. **Removing stop words.**\n",
    "5. **Stemming or Lemmatization.**\n",
    "6. **HTML Parsing and Cleanup.**\n",
    "7. **Spell Correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7d8e72-b584-4d33-a57b-21ad0b133ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We're LaRNING1 nATURAL-LANguage-Processing!üòÇüôáüçÜ\n",
      "In this\\ example wE are goINg to LeaRN various text9 preProcessing Steps.\n",
      "I'm GOiNG TO-Be Mr. Rich. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\DeLL\\AppData\\Local\\Temp\\ipykernel_3456\\2903916327.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  raw_text = '''\n"
     ]
    }
   ],
   "source": [
    "raw_text = '''\n",
    "We're LaRNING1 nATURAL-LANguage-Processing!üòÇüôáüçÜ\n",
    "In this\\ example wE are goINg to LeaRN various text9 preProcessing Steps.\n",
    "I'm GOiNG TO-Be Mr. Rich. \n",
    "'''\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17616cd6-338d-4163-8ade-c528cab7c91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07398a25-7126-4b9c-a5ee-e66cf9e3de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Were LaRNING nATURALLANguageProcessingüòÇüôáüçÜ\n",
      "In this example wE are goINg to LeaRN various text preProcessing Steps\n",
      "Im GOiNG TOBe Mr Rich \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ''.join([char for char in raw_text if char not in string.punctuation and not char.isdigit()])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab5e5f-cce1-4de7-a777-f8c6235e9adf",
   "metadata": {},
   "source": [
    "### A more powerful weapon to remove special characters and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8540b47-36e5-4c69-b028-1b4bc9d995c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We re LaRNING  nATURAL LANguage Processing!    In this  example wE are goINg to LeaRN various text  preProcessing Steps. I m GOiNG TO Be Mr. Rich.  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex = '[^a-zA-Z.!]'\n",
    "text = re.sub(regex, ' ', raw_text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53e8ae-b087-48ec-8c5d-e0cce20a8b69",
   "metadata": {},
   "source": [
    "### Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a20f81c-f7ab-4fc2-bd12-0eb640fa179f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' we re larning  natural language processing!    in this  example we are going to learn various text  preprocessing steps. i m going to be mr. rich.  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a683255-c77f-416a-90fb-8af5b2485eec",
   "metadata": {},
   "source": [
    "### # Tokenization (Sentence Tokenization and Word Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28eec6d6-5172-4ee1-b6e8-6ec3e3a3f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'we', 're', 'larning', '', 'natural', 'language', 'processing!', '', '', '', 'in', 'this', '', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', '', 'preprocessing', 'steps.', 'i', 'm', 'going', 'to', 'be', 'mr.', 'rich.', '', '']\n"
     ]
    }
   ],
   "source": [
    "words = text.split(' ')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e032072-4ce1-4584-a2ca-c9c65a621db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we re larning  natural language processing!    in this  example we are going to learn various text  preprocessing steps', ' i m going to be mr', ' rich', '  ']\n"
     ]
    }
   ],
   "source": [
    "sentences = text.split('.')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c56640c2-a243-433d-953a-dc55b017ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09bb1690-2b23-4331-83a3-c1f1cee09d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DeLL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d11b4a65-5348-45d2-ae97-4e827d6b6aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we re larning  natural language processing!', 'in this  example we are going to learn various text  preprocessing steps.', 'i m going to be mr. rich.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "my_sent = sent_tokenize(text)\n",
    "print(my_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bafa2f2-2442-4831-969e-925839aafbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'larning', 'natural', 'language', 'processing', '!', 'in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'i', 'm', 'going', 'to', 'be', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60fcae72-6375-4bdb-9a41-ff0e4738e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'larning', 'natural', 'language', 'processing', '!']\n",
      "['in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.']\n",
      "['i', 'm', 'going', 'to', 'be', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in sent_tokenize(text):\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a097540f-006f-4673-9c8b-6ab57c872ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5c3834d-9744-4ce1-b0f0-6cc5366a843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8407d4-656d-4c0e-9036-763208264c93",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d19d4f20-36ca-4401-a12a-90f695fdd104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larning', 'natural', 'language', 'processing', '!', 'example', 'going', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'going', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "words = [word for word in words if word not in stopwords.words('english')]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42a40a-9e3e-4e30-91f6-67d65adb9c22",
   "metadata": {},
   "source": [
    "### Stemming - Removing the suffix and reducing the word with the intention of taking the root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe644c56-2005-4172-a833-145463e193ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larn', 'natur', 'languag', 'process', '!', 'exampl', 'go', 'learn', 'variou', 'text', 'preprocess', 'step', '.', 'go', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "clean_tokens_stem = [stemmer.stem(word) for word in words]\n",
    "print(clean_tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6041e5-8d5f-47fe-9136-ed7ee378ae4b",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58df50b6-0fce-4705-bfaa-5d55d76989f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larning', 'natural', 'language', 'processing', '!', 'example', 'going', 'learn', 'various', 'text', 'preprocessing', 'step', '.', 'going', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_text_lemmatization = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(clean_text_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01818481-d31a-4e1d-8502-6cd81d1daf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "566eb34d-ab9d-4f7a-a5f7-6a951106a410",
   "metadata": {},
   "source": [
    "## Putting all the steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08763aea-6040-4870-8225-d5cb74107a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "text_data = ['We are learning machine learning $', 'Processing Natural - Language Data.', '10 machine - learning algorithms.', 'We are mimicking natural intelligence.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f56a27-94d9-460b-9ed6-108102e90302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are learning machine learning $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing Natural - Language Data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are mimicking natural intelligence.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text\n",
       "0      We are learning machine learning $\n",
       "1     Processing Natural - Language Data.\n",
       "2       10 machine - learning algorithms.\n",
       "3  We are mimicking natural intelligence."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'text' : text_data})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d789bbf1-8048-4592-8ece-488b857a0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    regex = '[^a-zA-Z!.]'\n",
    "    doc = re.sub(regex, ' ', doc)\n",
    "\n",
    "    doc = doc.lower()\n",
    "\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_token = [word for word in tokens if not word in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer_tokens = [lemmatizer.lemmatize(word) for word in filtered_token]\n",
    "\n",
    "    return ' '.join(lemmatizer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4023ca47-3099-4a0e-a14e-3aa4444ded68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are learning machine learning $</td>\n",
       "      <td>learning machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing Natural - Language Data.</td>\n",
       "      <td>processing natural language data .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms.</td>\n",
       "      <td>machine learning algorithm .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are mimicking natural intelligence.</td>\n",
       "      <td>mimicking natural intelligence .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text                          clean_text\n",
       "0      We are learning machine learning $           learning machine learning\n",
       "1     Processing Natural - Language Data.  processing natural language data .\n",
       "2       10 machine - learning algorithms.        machine learning algorithm .\n",
       "3  We are mimicking natural intelligence.    mimicking natural intelligence ."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].apply(lambda x : clean(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7eb62-ae8c-478e-aef3-15cf1158c289",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0072aa-9d85-44e2-9ef5-c13034a07518",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb532c53-872e-4715-83ca-5653bae3219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape of text_dtm (# of docs, # of unique vocabulary): (4, 9)\n",
      "Vocab : ['algorithm' 'data' 'intelligence' 'language' 'learning' 'machine'\n",
      " 'mimicking' 'natural' 'processing']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vect = CountVectorizer()\n",
    "text_dtm = bow_vect.fit_transform(df['clean_text'])\n",
    "print()\n",
    "print(f'shape of text_dtm (# of docs, # of unique vocabulary): {text_dtm.shape}')\n",
    "print(f'Vocab : {bow_vect.get_feature_names_out()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a53bc33-11e7-41a9-80fd-c1c81b87394b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>data</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>mimicking</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   algorithm  data  intelligence  language  learning  machine  mimicking  \\\n",
       "0          0     0             0         0         2        1          0   \n",
       "1          0     1             0         1         0        0          0   \n",
       "2          1     0             0         0         1        1          0   \n",
       "3          0     0             1         0         0        0          1   \n",
       "\n",
       "   natural  processing  \n",
       "0        0           0  \n",
       "1        1           1  \n",
       "2        0           0  \n",
       "3        1           0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(text_dtm.toarray(), columns = bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff381b27-e5be-484b-aa66-eba46b384f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b89001b-4326-41ee-8478-4f5d15cb6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Convert a collection of text documents to a matrix of token counts.','This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.','If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.','For an efficiency comparison of the different feature extractors.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9769e8c-2cef-44b1-a478-f499db2ebb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 61 stored elements and shape (4, 52)>\n",
      "  Coords\tValues\n",
      "  (0, 8)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 30)\t2\n",
      "  (0, 40)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 45)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 46)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 25)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 39)\t2\n",
      "  (1, 34)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 48)\t1\n",
      "  (1, 35)\t1\n",
      "  (1, 10)\t1\n",
      "  (2, 30)\t2\n",
      "  (2, 45)\t1\n",
      "  (2, 42)\t3\n",
      "  (2, 24)\t1\n",
      "  (2, 51)\t2\n",
      "  :\t:\n",
      "  (2, 38)\t1\n",
      "  (2, 26)\t1\n",
      "  (2, 20)\t1\n",
      "  (2, 36)\t1\n",
      "  (2, 43)\t1\n",
      "  (2, 29)\t1\n",
      "  (2, 21)\t1\n",
      "  (2, 50)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 49)\t1\n",
      "  (2, 37)\t1\n",
      "  (2, 23)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 11)\t1\n",
      "  (3, 30)\t1\n",
      "  (3, 42)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 20)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 19)\t1\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "text = cv.fit_transform(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54cfb2-6c4a-498c-8dff-ffde4ecdb8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
